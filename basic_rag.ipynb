{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bentoml>=1.2.2 sentence-transformers==2.2.2 sentencepiece==0.1.99 torch transformers==4.37.1 pymilvus>=2.3 octoai-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Spin up Bento Sentence Transformers Server. [Instructions here.](https://github.com/bentoml/BentoSentenceTransformers)\n",
    "2. Embed data via Bento and store in Milvus via [Milvus Docker](https://milvus.io/docs/install_standalone-docker-compose.md)\n",
    "3. Get LLM from [OctoAI](octoai.cloud)\n",
    "4. Do RAG\n",
    "\n",
    "Note: BentoSentenceTransformers already cloned into this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Spin up Bento Sentence Transformers Server. [Instructions here.](https://github.com/bentoml/BentoSentenceTransformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "\n",
    "bento_client = bentoml.SyncHTTPClient(\"http://localhost:3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: list) -> list:\n",
    "    if len(texts) > 25:\n",
    "        splits = [texts[x:x+25] for x in range(0, len(texts), 25)]\n",
    "        embeddings = []\n",
    "        for split in splits:\n",
    "            embedding_split = bento_client.encode(\n",
    "                sentences = split\n",
    "            )\n",
    "            for embedding in embedding_split:\n",
    "                embeddings.append(embedding)\n",
    "        return embeddings\n",
    "    return bento_client.encode(\n",
    "        sentences=texts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Embed data via Bento and store in Milvus via [Milvus Docker](https://milvus.io/docs/install_standalone-docker-compose.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"bmo_test\"\n",
    "connections.connect(host=\"localhost\", port=19530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "DIMENSION = 384\n",
    "\n",
    "# id and embedding are required to define\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "# \"enable_dynamic_field\" lets us insert data with any metadata fields\n",
    "schema = CollectionSchema(fields=fields, enable_dynamic_field=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection\n",
    "\n",
    "# define the collection name and pass the schema\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = {\n",
    "    \"index_type\": \"HNSW\", # one of 11 Milvus indexes\n",
    "    \"metric_type\": \"IP\", # L2, Cosine, or IP\n",
    "    \"params\": {\n",
    "        \"M\": 8, # higher M = consumes more memory but better search quality\n",
    "        \"efConstruction\": 64 # higher efConstruction = slower build, better search\n",
    "    }, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the field to index on and the parameters to index with\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "# load the collection into memory\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naively chunk on newlines\n",
    "def chunk_text(filename: str) -> list:\n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.read()\n",
    "    sentences = text.split(\"\\n\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = os.listdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store chunked text for each of the cities in a list of dicts\n",
    "city_chunks = []\n",
    "for city in cities:\n",
    "    chunked = chunk_text(f\"data/{city}\")\n",
    "    cleaned = []\n",
    "    for chunk in chunked:\n",
    "        if len(chunk) > 7:\n",
    "            cleaned.append(chunk)\n",
    "    mapped = {\n",
    "        \"city_name\": city.split(\".\")[0],\n",
    "        \"chunks\": cleaned\n",
    "    }\n",
    "    city_chunks.append(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for city_dict in city_chunks:\n",
    "    embedding_list = get_embeddings(city_dict[\"chunks\"]) # returns a list of lists\n",
    "    # now match texts with embeddings and city name\n",
    "    for i, embedding in enumerate(embedding_list):\n",
    "        entry = {\"embedding\": embedding,\n",
    "                 \"sentence\": city_dict[\"chunks\"][i], # poorly named cuz it's really a bunch of sentences, but meh\n",
    "                 \"city\": city_dict[\"city_name\"]}\n",
    "        entries.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.insert(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Get LLM from [OctoAI](octoai.cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OCTOAI_TOKEN\"] = os.getenv(\"OCTOAI_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octoai.client import Client\n",
    "\n",
    "octo_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dorag(question: str, context: str):\n",
    "\n",
    "    completion = octo_client.chat.completions.create(\n",
    "    messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a helpful assistant. The user has a question. Answer the user question based only on the context: {context}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{question}\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"nous-hermes-2-mixtral-8x7b-dpo\",\n",
    "        max_tokens=512,\n",
    "        presence_penalty=0,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return completion.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_a_question(question):\n",
    "    embeddings = get_embeddings([question])\n",
    "    res = collection.search(\n",
    "        data=embeddings,  # search for the one (1) embedding returned as a list of lists\n",
    "        anns_field=\"embedding\",  # Search across embeddings\n",
    "        param={\"metric_type\": \"IP\",\n",
    "                \"params\": {\"ef\": 16}},\n",
    "        limit = 5,  # get me the top 3 results\n",
    "        output_fields=[\"sentence\"]  # get the sentence/chunk and city\n",
    "    )\n",
    "    sentences = []\n",
    "    for hits in res:\n",
    "        for hit in hits:\n",
    "            sentences.append(hit.entity.get(\"sentence\"))\n",
    "    context = \". \".join(sentences)\n",
    "    return dorag(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_a_question(\"What state is Cambridge in?\")[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "# if utility.has_collection(COLLECTION_NAME):\n",
    "#     utility.drop_collection(COLLECTION_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
